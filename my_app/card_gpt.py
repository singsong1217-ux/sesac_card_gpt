import streamlit as st
from card_rag import search_card
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv

load_dotenv()

# ======================== session_state ì„¤ì • ===========================
# memory ê°ì²´ëŠ” ì„¸ì…˜ì— ì €ì¥
if "pre_memory" not in st.session_state:
    st.session_state["pre_memory"] = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True   # <-- ìˆ˜ì •: return_messages (ë³µìˆ˜)
    )
    
# í™”ë©´ì— ì¶œë ¥í•  ëŒ€í™” ê¸°ë¡ (í‚¤ë¥¼ "messages"ë¡œ í†µì¼)
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” AI Assistant ì…ë‹ˆë‹¤."}
    ]

# ======================== model & prompt ì„¤ì • =========================== 
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

system_prompt = """
ë„ˆëŠ” ì¹´ë“œì‚¬ ì§ì›ì´ì•¼. ê³ ê°ì˜ ì§ˆì˜ê°€ ë“¤ì–´ì˜¤ë©´ contextì— ë”°ë¼ ê°€ì¥ í˜œíƒì´ ë§ì€ ì¹´ë“œë¥¼ 3ê°œ ì¶”ì²œí•´ì¤˜. 
context ë‚´ìš©ì— í•œí•´ì„œë§Œ ì¶”ì²œí•´ì£¼ë˜, contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°œì„¤í•˜ì§€ ë§ì•„ì¤˜. 
contextë¥¼ ì°¸ê³ í•œ ì¶œë ¥ í¬ë§·ì€ ì•„ë˜ì™€ ê°™ì•„.

--ì¶œë ¥ í¬ë§·--
ğŸ“Œ í•´ë‹¹ë€ì— ë¨¼ì € ì‚¬ìš©ìê°€ ì–´ë–¤ ì¹´ë“œë¥¼ ì›í•˜ëŠ”ì§€ íŒŒì•…í•´ì„œ ìš”ì•½ë³¸ì„ í•œ ì¤„ë¡œ ì‘ì„±í•´ì¤˜.
ğŸ’³ ì¶”ì²œì¹´ë“œëª…
    - ì¶”ì²œ ì´ìœ 
    - í•´ë‹¹ ì¹´ë“œì˜ í˜œíƒ
ğŸ’³ ì¶”ì²œì¹´ë“œëª…
    - ì¶”ì²œ ì´ìœ 
    - í•´ë‹¹ ì¹´ë“œì˜ í˜œíƒ
ğŸ’³ ì¶”ì²œì¹´ë“œëª…
    - ì¶”ì²œ ì´ìœ 
    - í•´ë‹¹ ì¹´ë“œì˜ í˜œíƒ
"""

user_prompt = """\
ì•„ë˜ì˜ ì‚¬ìš©ì questionì„ ì½ê³  contextë¥¼ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´ë“œ(ì‚¬ìš©ìê°€ í˜œíƒì„ ìµœëŒ€ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹´ë“œ)ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    
--chat_history--
{chat_history}
    
--question--
{question}
    
--context--
{context}
"""
    
final_prompt = ChatPromptTemplate([
    ("system", system_prompt),
    ("user", user_prompt)
])
    
# ì‚¬ìš©ì ì…ë ¥ê°’ì„ ë°›ì•„ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ì •ì˜ (í•¨ìˆ˜ëª… ì˜¤íƒ€ ìˆ˜ì •)
def get_user_input(question):
    return {
        # memoryì—ì„œ ì˜¬ë°”ë¥¸ ì†ì„±ëª… ì‚¬ìš© (.chat_memory.messages)
        "chat_history": st.session_state["pre_memory"].chat_memory.messages,
        "question": question,
        "context": search_card()
    }

chain = RunnableLambda(get_user_input) | final_prompt | model | StrOutputParser()

# ëŒ€í™”ë‚´ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡í•´ì£¼ëŠ” í•¨ìˆ˜ ì •ì˜
def conversation_with_memory(question):
    # 1. ë©”ì„¸ì§€ ì¶œë ¥ ê³µê°„ ìƒì„±
    stream_placeholder = st.empty()
    
    # 2. ì‘ë‹µ ìƒì„± ë° ì¶œë ¥ (stream ì‚¬ìš©)
    full_response = ""
    for chunk in chain.stream(question):
        full_response += chunk
        stream_placeholder.write(full_response)
        
    # 3. ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ai ì‘ë‹µì„ memoryì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
    st.session_state["pre_memory"].save_context(
        {"input": question},
        {"output": full_response}
    )
    
    # 4. session_state["messages"]ì— ì €ì¥í•  ìš©ë„ë¡œ full_response ë°˜í™˜
    return full_response 

# ======================== ë©”ì¸í™”ë©´ ì„¤ì • ===========================
st.title("My GPT")

# 1. ëŒ€í™” ê¸°ë¡ ì¶œë ¥ (messagesë¡œ í†µì¼)
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        
# 2. ì‚¬ìš©ì ì§ˆì˜ ì‘ì„±
question = st.chat_input("ì‚¬ìš©ì ì…ë ¥")

# 3. ì‚¬ìš©ì ì§ˆì˜ ì €ì¥ & ì¶œë ¥
if question:
    st.session_state["messages"].append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.write(question)

# 4. ai ë‹µë³€ ìƒì„± & ì¶œë ¥
if st.session_state["messages"][-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        try:
            ai_response = conversation_with_memory(question)
            # st.question_state -> st.session_state, key "messages"ë¡œ append
            st.session_state["messages"].append({"role": "assistant", "content": ai_response})
        except Exception as e:
            error_ = f"""\
ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë©”ì„¸ì§€ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.

ë°œìƒ ì—ëŸ¬: {e}
"""
            st.error(error_)
